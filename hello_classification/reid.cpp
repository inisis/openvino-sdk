#include "reid.h"
#include "samples/ocv_common.hpp"

using namespace InferenceEngine;

ReID::~ReID() {

}

void ReID::init(const std::string &modelPath, int gpuID, int decrypt_model, bool support_nv12) {
    std::cout << "--------------------------- 1. Load Plugin for inference engine -------------------------------------" << std::endl;
   plugin_ = InferencePlugin(PluginDispatcher().getSuitablePlugin(TargetDevice::eCPU));
    // -----------------------------------------------------------------------------------------------------

    std::cout << "--------------------------- 2. Read IR Generated by ModelOptimizer (.xml and .bin files) ------------" << std::endl;
    network_reader_.ReadNetwork(fileNameToString(modelPath));
    network_reader_.ReadWeights(fileNameToString(modelPath).substr(0, modelPath.size() - 4) + ".bin");
    network_reader_.getNetwork().setBatchSize(1);
    network_ = network_reader_.getNetwork();
    // -----------------------------------------------------------------------------------------------------

    std::cout << "--------------------------- 3. Configure input & output ---------------------------------------------" << std::endl;
    std::cout << "--------------------------- Prepare input blobs -----------------------------------------------------" << std::endl;
    input_info_ = network_.getInputsInfo().begin()->second;
    input_name_ = network_.getInputsInfo().begin()->first;

    input_info_->setLayout(Layout::NCHW);
    input_info_->setPrecision(Precision::U8);

    std::cout << "--------------------------- Prepare output blobs ----------------------------------------------------" << std::endl;
    output_info_ = network_.getOutputsInfo().begin()->second;
    output_name_ = network_.getOutputsInfo().begin()->first;

    output_info_->setPrecision(Precision::FP32);
    // -----------------------------------------------------------------------------------------------------

    std::cout << "--------------------------- 4. Loading model to the plugin ------------------------------------------" << std::endl;

    networkConfig_[PluginConfigParams::KEY_CPU_THREADS_NUM] = std::to_string(1);
    executable_network_ = plugin_.LoadNetwork(network_, networkConfig_);

    std::cout << "--------------------------- 5. Create infer request -------------------------------------------------" << std::endl;
    infer_request_ = executable_network_.CreateInferRequest();
    Blob::Ptr input = infer_request_.GetBlob(input_name_);
    std::cout << input_name_ << std::endl;
    std::cout << &infer_request_ << std::endl;
}

void ReID::set_model_type(int model_type) {
    std::cout << "Mode: " << model_type << std::endl;
}

void ReID::classify(const std::vector<cv::Mat> &images, std::vector<std::vector<float>> &classify_results) {
    for(int i = 0; i < images.size(); ++i){
        std::vector<float> classify_result;
        classify_single(images[i], classify_result);
        classify_results.push_back(classify_result);
    }
}


void ReID::classify_single(const cv::Mat& image, std::vector<float> &classify_result) {
    std::cout << "--------------------------- 6. Prepare input --------------------------------------------------------" << std::endl;
    /* Resize manually and copy data from the image to the input blob */
    cv::Mat small_image;
    cv::resize(image, small_image, cv::Size{112, 112});

    Blob::Ptr imgBlob = wrapMat2Blob(small_image);
    infer_request_.SetBlob(input_name_, imgBlob);
//    Blob::Ptr input = infer_request_.GetBlob(input_name_);
//    std::cout << input_name_ << std::endl;
//    auto input_data = input->buffer().as<PrecisionTrait<Precision::U8>::value_type *>();
//
//    size_t channels_number = input->getTensorDesc().getDims()[1];
//    size_t image_size = input->getTensorDesc().getDims()[3] * input->getTensorDesc().getDims()[2];
//
//    for (size_t pid = 0; pid < image_size; ++pid) {
//        for (size_t ch = 0; ch < channels_number; ++ch) {
//            input_data[ch * image_size + pid] = 1;
//        }
//    }

    std::cout << "--------------------------- 7. Do inference --------------------------------------------------------" << std::endl;
    /* Running the request synchronously */
    const auto startTime = std::chrono::high_resolution_clock::now();
    for(int i = 0; i < 10; ++i)
    {
        infer_request_.Infer();
    }
    double totalDuration = std::chrono::duration_cast<std::chrono::nanoseconds>(std::chrono::high_resolution_clock::now() - startTime).count() * 0.000001;
    std::cout << "time cost single: " << totalDuration << "ms" << std::endl;
    // -----------------------------------------------------------------------------------------------------

    std::cout << "--------------------------- 8. Process output ------------------------------------------------------" << std::endl;
    Blob::Ptr output = infer_request_.GetBlob(output_name_);
    auto output_size = output->size();
    for(unsigned int i = 0; i < output_size; ++i)
    {
        const auto result = output->buffer().as<InferenceEngine::PrecisionTrait<InferenceEngine::Precision::FP32>::value_type*>()[i];
        classify_result.push_back(result);
    }
    // Print classification results
    ClassificationResult classificationResult(output, {fileNameToString("test.txt")}, 1, 128);
    classificationResult.print();

}

void ReID::release() {
    std::cout << "Released" << std::endl;
}