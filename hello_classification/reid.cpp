#include "fstream"
#include "reid.h"
#include "samples/ocv_common.hpp"

using namespace InferenceEngine;

ReID::~ReID() {

}

void ReID::init(const std::string &modelPath, int gpuID, int decrypt_model, bool support_nv12) {
    std::cout << "--------------------------- 1. Load Plugin for inference engine -------------------------------------" << std::endl;
   plugin_ = InferencePlugin(PluginDispatcher().getSuitablePlugin(TargetDevice::eCPU));
    // -----------------------------------------------------------------------------------------------------

    std::cout << "--------------------------- 2. Read IR Generated by ModelOptimizer (.xml and .bin files) ------------" << std::endl;
    network_reader_.ReadNetwork(fileNameToString(modelPath));
    network_reader_.ReadWeights(fileNameToString(modelPath).substr(0, modelPath.size() - 4) + ".bin");
    network_reader_.getNetwork().setBatchSize(1);
    network_ = network_reader_.getNetwork();
    // -----------------------------------------------------------------------------------------------------

    std::cout << "--------------------------- 3. Configure input & output ---------------------------------------------" << std::endl;
    std::cout << "--------------------------- Prepare input blobs -----------------------------------------------------" << std::endl;
    input_info_ = network_.getInputsInfo().begin()->second;
    input_name_ = network_.getInputsInfo().begin()->first;

    input_info_->setLayout(Layout::NCHW);
    input_info_->setPrecision(Precision::FP32);

    std::cout << "--------------------------- Prepare output blobs ----------------------------------------------------" << std::endl;
    output_info_ = network_.getOutputsInfo().begin()->second;
    output_name_ = network_.getOutputsInfo().begin()->first;

    output_info_->setPrecision(Precision::FP32);
    // -----------------------------------------------------------------------------------------------------

    std::cout << "--------------------------- 4. Loading model to the plugin ------------------------------------------" << std::endl;

    networkConfig_[PluginConfigParams::KEY_CPU_THREADS_NUM] = std::to_string(1);
    executable_network_ = plugin_.LoadNetwork(network_, networkConfig_);

    std::cout << "--------------------------- 5. Create infer request -------------------------------------------------" << std::endl;
    infer_request_ = executable_network_.CreateInferRequest();
    Blob::Ptr input = infer_request_.GetBlob(input_name_);
}

void ReID::set_model_type(int model_type) {
    std::cout << "Mode: " << model_type << std::endl;
}

void ReID::classify(const std::vector<cv::Mat> &images, std::vector<std::vector<float>> &classify_results) {
    for(int i = 0; i < images.size(); ++i){
        std::vector<float> classify_result;
        classify_single(images[i], classify_result);
        classify_results.push_back(classify_result);
    }
}


void ReID::classify_single(const cv::Mat& image, std::vector<float> &classify_result) {
    std::cout << "--------------------------- 6. Prepare input --------------------------------------------------------" << std::endl;
    /* Resize manually and copy data from the image to the input blob */
    cv::Mat output_float;
    image.convertTo(output_float, CV_32FC3);

    cv::Mat small_image;
    cv::resize(output_float, small_image, cv::Size{112, 112});
    cv::Mat rgbMat;
    cv::cvtColor(small_image, rgbMat, cv::COLOR_BGR2RGB);
    cv::Mat input_image = (rgbMat - cv::Scalar{127.5, 127.5, 127.5}) / 128.0f;

    Blob::Ptr input = infer_request_.GetBlob(input_name_);
    auto input_data = input->buffer().as<PrecisionTrait<Precision::FP32>::value_type *>();

    size_t channels_number = input->getTensorDesc().getDims()[1];
    size_t image_size = input->getTensorDesc().getDims()[3] * input->getTensorDesc().getDims()[2];

    float *outputPtr = input_image.ptr<float>(0);

    for (size_t ch = 0; ch < channels_number; ++ch) {
        for (size_t pid = 0; pid < image_size; ++pid) {
            input_data[ch * image_size + pid] = outputPtr[ch + pid * channels_number];
        }
    }

    std::cout << "--------------------------- 7. Do inference --------------------------------------------------------" << std::endl;
    /* Running the request synchronously */
    const auto startTime = std::chrono::high_resolution_clock::now();

    infer_request_.Infer();

    double totalDuration = std::chrono::duration_cast<std::chrono::nanoseconds>(std::chrono::high_resolution_clock::now() - startTime).count() * 0.000001;
    std::cout << "time cost single: " << totalDuration << "ms" << std::endl;
    // -----------------------------------------------------------------------------------------------------

    std::cout << "--------------------------- 8. Process output ------------------------------------------------------" << std::endl;
    Blob::Ptr output = infer_request_.GetBlob(output_name_);
    auto output_size = output->size();
    std::vector<float> vec;
    for(unsigned int i = 0; i < output_size; ++i)
    {
        const auto result = output->buffer().as<InferenceEngine::PrecisionTrait<InferenceEngine::Precision::FP32>::value_type*>()[i];
        vec.push_back(result);
    }

    cv::normalize(vec, classify_result, 1.0, 0.0, cv::NORM_L2);
}

void ReID::release() {
    std::cout << "Released" << std::endl;
}